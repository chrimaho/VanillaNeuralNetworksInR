---
title: 'Vanilla Neural Networks in R'
subtitle: 'Designing and Building Neural Networks from Scratch in R, using no Deep Learning Frameworks'
author: 'Author: [Chris Mahoney](https://www.linkedin.com/in/chrimaho/)'
date: 'Special thanks to: Alex Scriven'
toc-title: Contents
output:
    html_notebook: 
        code_folding: none
        highlight: haddock
        includes:
            after_body: footer.html
            in_header: header.html
        number_sections: yes
        theme: lumen
        toc: yes
        toc_depth: 4
        toc_float:
            collapsed: no
    html_document:
        code_download: yes
        highlight: haddock
        number_sections: yes
        template: default_toc.html
        theme: lumen
        toc: yes
        toc_depth: 4
        toc_float:
            collapsed: no
        includes:
            in_header: header.html
            after_body: footer.html
---

<style>
.math {
    <!-- font-size: 120%; -->
    font-style: normal;
    font-family: "Cambria Math";
}
.column {
    float: left;
    width: 50%;
    border: 1px solid black;
}
.row:after {
    content: "";
    display: table;
    clear: both;
}
h1, .h1 {
    margin-top: 40px;
    font-weight: bold;
}
h2, .h2 {
    margin-top: 40px;
    margin-left: 40px;
}
</style>


# Introduction {#introduction}

Modern-day Data Science techniques frequently use robust frameworks for designing and building machine learning solutions. In the [`R`](https://www.r-project.org/) community, packages such as the [`tidyverse`](https://www.tidyverse.org/) and the [`caret`](http://caret.r-forge.r-project.org/) packages are frequently referenced; and within [`Python`](), packages such as [`numpy`](https://numpy.org/), [`pandas`](https://pandas.pydata.org/), [`sci-kit learn`](https://scikit-learn.org/) are frequently referenced. There are even some packages that have been built to be used in either language, such as [`keras`](https://keras.io/), [`pytorch`](https://pytorch.org/), [`tensorflow`](https://www.tensorflow.org/). However, the limitation of using these packages is the '_blackbox_' phenomenon, where users do not understand what is happening behind-the-scenes (or 'under the hood', if you will). Users know how to use the functions, and can interpret the results, but don't necessarily know how the packages were able to achieve the results.

The purpose of this paper is to create a 'back to basics' approach to designing Deep Learning solutions. The intention is not to create the most predictive model, nor is it to use the latest and greatest techniques (such as convolution or recursion); but the intention is to create a _basic_ neural network, from _scratch_, using _no_ frameworks, and to walk through the methodology.

**Note**: The word '_Vanilla_' in 'Vanilla Neural Networks' simply refers to the fact it is built from scratch, and does not use any pre-existing frameworks in its construction.

# Context {#context}

There are already many websites and blogs which explain how this process is done. Such as [Jason Brownlee](https://machinelearningmastery.com/about/)'s article [_How to Code a Neural Network with Backpropagation In Python (from scratch)_](https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/), and [DeepLearning.ai](https://notebooks.azure.com/goldengrape/projects/deeplearning-ai)'s notebook [_dnn_app_utils_v2.py_](https://notebooks.azure.com/goldengrape/projects/deeplearning-ai/html/COURSE%201%20Neural%20Networks%20and%20Deep%20Learning/Week%204/Deep%20Neural%20Network%20Application_%20Image%20Classification/dnn_app_utils_v2.py) (on the [Microsoft Azure Notebooks](https://notebooks.azure.com/) network). However, these sources are all written in `Python`. Which is fine, if that's what is needed, and there are some very legitimate reasons to use `Python` over other languages. But this paper will be written in `R`.

The `R` language was chosen for two reasons:

1. It is the language that I am familiar with. Noting that I can speak `Python` (along with other languages); I just chose `R` to show how it can be achieved using this language.
1. To prove that there are many different ways to achieve the same outcome. So, while there are sometimes legitimate constraints for choosing one language over another (business legacy, technological availability, system performance, etc), sometimes one language is chosen simply because it is stylistically more preferable.

Therefore, let's see how to architect and construct a Vanilla Neural Network in `R`.

# Layout {#layout}

**Discuss how this notebook is set up**

# Set Up {#setup}

## Load Packages {#loadpackages}



```{r Load Packages, eval=TRUE, echo=TRUE}
library(tensorflow) #<-- Only used for getting the data. Nothing else
library(tidyverse)  #<-- Used for accessing various tools
library(magrittr)   #<-- Extends the `dplyr` syntax
library(grDevices)  #<-- For plotting the images
library(assertthat) #<-- Function assertions
library(roxygen2)   #<-- Documentation is important
library(caret)      #<-- Doing data partitioning
library(stringi)    #<-- Some string manipulation parts
library(DescTools)  #<-- The only package that properly checks `is.integer`
library(tictoc)     #<-- Time how long different processes take
```


```{r Load Custom Functions, eval=TRUE, echo=FALSE}
str_Format <- function(string, ...) {
    #' @title String Formatter
    #' @description Take an input string, and substitute in-string variables.
    #' @note This is similar to the Python `string.foramt()` method.
    #' @param string string. The string to be re-formatted. Note, each of the named arguments must be surrounded in curly brackets.
    #' @param ... variables. A list of variables. Note, these can either be named or not; but they must all be named, or all be blank, because it cannot handle a mixture. Each of these arguments must align to the variables in curly brackets from the `string` argument. These will be combined in to a list.
    #' @return A formatted string
    #' @example str_Format("Sammy the {animal} {verb} a {noun}.", animal="shark", verb="made", noun="house")
    #' @example str_Format("Sammy the {} {} a {}.", "shark", "made", "house")
    #' @example "Sammy the {animal} {verb} a {noun}" %>% str_Format(animal="shark", verb="made", noun="house")
    #' @references https://stackoverflow.com/questions/44763056/is-there-an-r-equivalent-of-pythons-string-format-function#answer-44763659
    #' @author chrimaho
    
    # Import packages ----
    require(stringr)
    require(magrittr)
    require(dplyr)
    require(assertthat)
    require(dynutils)
    require(english)
    
    # Validations ----
    assert_that(is.string(string))
    assert_that(c("stringr", "magrittr", "dplyr", "assertthat", "dynutils", "english") %all_in% .packages(), msg="The packages must be mounted.")
    
    # Set Up ----
    num_variables <- str_count(string, coll("{}"))
    vars <- list(...)
    
    # Handle if vars are not named ----
    if (num_variables>0) {
        
        # Add number in between each curly bracket
        for (i in 1:num_variables) {
            string %<>% str_replace(coll("{}"), paste0("{",as.english(i),"}"))
        }
        
        # Name the vars as numbers
        vars %<>% set_names(as.english(1:num_variables))
        
    }
    
    # Make environment ----
    envir <- as.environment(vars)
    parent.env(envir) <- .GlobalEnv
    
    # Perform substitution
    string %<>% str_replace_all("\\{", "${")
    str_return <- str_interp(string=string, env=envir)
    
    # Return ----
    return(str_return)
    
}


get_CountOfElementsWithCondition <- function(list_of_elements, condition=NULL) {
    #' @title Get Count of Elements With Condition
    #' @description Get the count of the number of elements in a list that meet a specified condition.
    #' @note The `condition` must be a hidden function. Also note that the `warnings` are suppressed when running the `condition` function.
    #' @param list_of_elements vector. The list of elements to check.
    #' @param condition function. The condition for checking. Must be a hidden function.
    #' @return An integer.
    #' @example get_CountOfElementsWithCondition(c("i", "1", "2", "3", "o"), function(x){IsWhole(as.numeric(x))}) ## returns 3
    #' @example get_CountOfElementsWithCondition(c("i", "1", "2", "3", "o"))                                      ## returns 5
    #' @example get_CountOfElementsWithCondition(c("i", "1", "2", "3", "o"), 2)                                   ## throws error
    #' @references https://thispointer.com/python-count-elements-in-a-list-that-satisfy-certain-conditions/#crayon-5ea195c434f39109077492-1
    #' @author chrimaho
    
    # Validations
    assert_that(is_vector(list_of_elements))
    assert_that(or(is_function(condition), is.null(condition)), msg="'condition' must either be a function or the value 'NULL'.")
    
    # Do work
    if (!is.null(condition)) {
        count <- sum(suppressWarnings(condition(list_of_elements)), na.rm=TRUE)
    } else {
        count <- length(list_of_elements)
    }
    
    # Return
    return(count)
}
```


## Get Data {#getdata}

```{r Get Data, eval=TRUE, echo=TRUE}
# Steps:
# 1. Get data from Tensorflow
# 2. Extract the second element (the test data)
# 3. Re-name the objects

# Run process
cifa <- tf$keras$datasets$cifar10$load_data() %>% 
    extract2(2) %>% 
    set_names(c("images","classes"))
```

```{r Define Class List, eval=TRUE, echo=TRUE}
# Source: https://github.com/EN10/CIFAR#classes
ClassList <- list(
    "0"="airplane",
    "1"="automobile",
    "2"="bird",
    "3"="cat",
    "4"="deer",
    "5"="dog",
    "6"="frog",
    "7"="horse",
    "8"="ship",
    "9"="truck"
)
```

```{r Check Data, eval=TRUE, echo=FALSE, results="asis"}
get_ObjectAttributes <- function(object, name) {
    #' @title Get Attributes
    #' @description Extract and print the key attributes of an object, including `name`, `class`, `type`, `mode`, `dims`.
    #' @note Add a note for the developer.
    #' @param object any. The object to be checked.
    #' @param name string. The name of the object.
    #' @return Nothing being returned
    #' @author chrimaho
    
    # Get attributes
    if(missing(name)) {
        name <- deparse(substitute(object))
    }
    class <- class(object)
    type <- typeof(object)
    mode <- mode(object)
    dims <- dim(object)
    if(!length(dims)>0){
        dims <- length(object)
    }
    
    # Print attributes
    "Object `{}`:  \n - class : `{}`  \n - type  : `{}`  \n - mode  : `{}`  \n - dims  : `{}`\n" %>% 
        str_Format(
            name,
            class,
            type,
            mode,
            dims
        ) %>% 
        str_remove_all("\"") %>% 
        cat
    
    return(invisible(NULL))
}

# Print result
get_ObjectAttributes(cifa$images)
```

For this, the custom function `str_Format()` is used. The source code for this function can be found [here]().

## Check Images {#checkimages}

```{r Extract Images, eval=TRUE, echo=TRUE}
set_MakeImage <- function(images, index=1) {
    #' @title Slice Array
    #' @description Slice the images at a given index
    #' @note Add a note for the developer.
    #' @seealso https://stackoverflow.com/questions/32113942/importing-cifar-10-data-set-to-r#answer-39672323
    #' @param images array. The array to be sliced.
    #' @param index int. The integer to be sliced
    #' @return An `rgb()` object.
    #' @author chrimaho
    
    # Libraries ----
    require(dplyr)
    require(grDevices)
    require(assertthat)
    
    # Validations ----
    assert_that(is.array(images))
    assert_that(is.count(index))
    assert_that(images %>% dim %>% length == 4, msg="'images' must have 4 dimensions.")
    
    # Extract elements ----
    img <- images[index,,,]
    img.r <- img[,,1]
    img.g <- img[,,2]
    img.b <- img[,,3]
    
    # Make rgb ----
    img.rgb <- rgb(img.r, img.g, img.b, maxColorValue=255)
    
    # Fix dimensions ----
    dim(img.rgb) <- dim(img.r)
    
    # Return ----
    return(img.rgb)
}

plt_PlotImage <- function(images, classes, index=1) {
    #' @title Plot Array
    #' @description Plot a given image.
    #' @note Add a note for the developer.
    #' @seealso https://stackoverflow.com/questions/12918367/how-to-plot-with-a-png-as-background#answer-12918368
    #' @param images array. The image to be plotted.
    #' @param index int. The index of the image to be plotted.
    #' @return Nothing is returned.
    #' @author chrimaho
    
    # Libraries ----
    require(dplyr)
    require(grDevices)
    require(assertthat)
    require(grid)
    
    # Validations ----
    assert_that(is.array(images))
    assert_that(is.count(index))
    assert_that(images %>% dim %>% length == 4, msg="'images' must have 4 dimensions.")
    
    # Slice images ----
    img <- set_MakeImage(images, index)
    lbl <- classes[index] %>% as.character() %>% ClassList[[.]]
    
    # Plot image ----
    plot.new()
    lim <- par()
    rasterImage(img, lim$usr[1], lim$usr[3], lim$usr[2], lim$usr[4], interpolate=FALSE)
    title(lbl, font.main=2)
    
    # Return ----
    return(invisible(NULL))
}
```

```{r View Images, eval=TRUE, echo=TRUE}
for (i in 1:20) {
    plt_PlotImage(cifa$images, cifa$classes, i)
}
```

## Fix data {#fixdata}

```{r}
cifa %>% extract2("classes") %>% head(100) %>% as.vector()
cifa$classes <- ifelse(cifa$classes==1,1,0)
cifa %>% extract2("classes") %>% head(100) %>% as.vector()
```

## Split Data {#splitdata}

```{r Split Data, eval=TRUE, echo=TRUE}
set.seed(1234)
partition <- createDataPartition(cifa$classes, p=0.7, list=FALSE)
trn_img <- cifa$images[partition,,,]
tst_img <- cifa$images[-partition,,,]
trn_cls <- cifa$classes[partition]
tst_cls <- cifa$classes[-partition]
rm(partition)
```

```{r Check Splitting, eval=TRUE, echo=TRUE}
for (i in c("trn_img","tst_img","trn_cls","tst_cls")) {
    j <- get(i)
    get_ObjectAttributes(j, i)
}
rm(i,j)
```

## Reshape Data {#reshapedata}

```{r Reshape Data, eval=TRUE, echo=TRUE}
trn_img <- array(trn_img, dim=c(dim(trn_img)[1], prod(dim(trn_img)[2:4])))
tst_img <- array(tst_img, dim=c(dim(tst_img)[1], prod(dim(tst_img)[2:4])))
trn_cls <- array(trn_cls, dim=c(1,length(trn_cls))) %>% t()
tst_cls <- array(tst_cls, dim=c(1,length(tst_cls))) %>% t()
```

```{r Check Data Shapes, eval=TRUE, echo=TRUE}
for (i in c("trn_img","tst_img","trn_cls","tst_cls")) {
    print(c(i %s+% ":", i %>% get %>% dim %>% paste))
}
```

## Standardise Data {#standardisedata}

```{r Standardise Data, eval=TRUE, echo=TRUE}
trn_img <- trn_img/255
tst_img <- tst_img/255
```

# Instantiate the Network {#instantiatenetwork}

## Set the Architecture

- 5 hidden layers
- 7 total layers
    1. input = 3072
    2. h1 = 100
    3. h2 = 75
    4. h3 = 50
    5. h4 = 30
    6. h5 = 20
    7. output = 10
- network is a list of lists
- each layer is a list
- all elements of the all lists are names

```{r Instantiate Network Definition, eval=TRUE, echo=TRUE}
InstantiateNetwork <- function(input=50, hidden=c(30,20,10), output=10) {
    #' @title Add function title
    #' @description Add function description.
    #' @note Add a note for the developer.
    #' @param input integer. What is Input1?
    #' @param hidden vector of integers. What is Input1?
    #' @param output integer. What is Input1?
    #' @return What is being returned?
    #' @author chrimaho
    
    # Validations ----
    assert_that(IsWhole(input))
    assert_that(is_vector(hidden))
    assert_that(all(IsWhole(hidden)))
    assert_that(IsWhole(output))
    
    # Set up ----
    model = list()
    names = c(
        "input",
        1:length(hidden),
        "output"
    )
    
    # Loop ----
    for (layer in names) {
        
        # Make layer
        model[[layer]] <- list(
             "nodz"      = ""  #<-- Number of nodes in this layer.
            ,"inpt"      = ""  #<-- Input matrix. Aka 'A_prev'. This is a duplicate of the activation of the previous layer, so for large networks this needs to be taken in to consideration.
            ,"wgts"      = ""  #<-- Weights matrix. Aka 'W'.
            ,"bias"      = ""  #<-- Bias vector. Aka 'b'.
            ,"linr"      = ""  #<-- Linear matrix. Aka 'Z'. This is the result of the linear algebra between inpt, wgts and bias.
            ,"acti"      = ""  #<-- Activation matrix. Aka 'A'. The result of applying an activation function to the linr matrix.
            ,"acti_func" = ""  #<-- The activation function used.
            ,"cost"      = ""  #<-- The overall cost of the model. This is a single value (the overall cost of the model), but is copied to each layer of the model.
            ,"back_cost" = ""  #<-- Gradient of the cost vector. Aka 'dA_cost'.
            ,"back_acti" = ""  #<-- Gradient of the Activation matrix. Aka 'dA'. The result of differentiation after having applied back propagation. with a given cost function.
            ,"back_linr" = ""  #<-- Gradient of the Linear algebra matrix. Aka 'dZ'. The result of backwards linear differentiation back propagation.
            ,"back_wgts" = ""  #<-- Gradient of the Weights matrix. Aka 'dW'. Also the result of back-prop.
            ,"back_bias" = ""  #<-- Gradient of the Bias vector. Aka 'db'. Also the result of back-prop.
        )
        
        # Set nodes
        if (layer=="input") {
            model[[layer]][["nodz"]] = input
        } else if (layer=="output") {
            model[[layer]][["nodz"]] = output
        } else {
            layer_index = layer %>% as.numeric()
            model[[layer]][["nodz"]] = hidden[layer_index]
        }
        
    }
    
    # Return ----
    return(model)
}
```

```{r Instantiate Network, eval=TRUE, echo=TRUE}
network_model <- InstantiateNetwork(
    input=3072, 
    hidden=c(100,75,50,30,20), 
    output=1
)
```

# Initialise the Network {#initialisenetwork}

## Weight Initialisation

```{r Weight Initialisation, eval=TRUE, echo=TRUE}
InitialiserXavier <- function(nodes_in, nodes_out, order=6) {
    #' @title Xavier Initialisation
    #' @description Initialise the weights based on Xavier initialisation
    #' @note Add a note for the developer.
    #' @references https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79
    #' @references http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
    #' @param nodes_in integer. The number of nodes coming in to this layer (ie. number of nodes in previous layer).
    #' @param nodes_out integer. The number of nodes outgoing from this layer (ie. number of nodes in this layer).
    #' @param order integer. The order of magnitude for this equation. Defaults to `6`.
    #' @return A floating point number.
    #' @author chrimaho
    
    # Validations ----
    assert_that(IsWhole(nodes_in))
    assert_that(IsWhole(nodes_out))
    assert_that(IsWhole(order))
    
    # Do work ----
    numer <- sqrt(order)
    denom <- sqrt(nodes_in + nodes_out)
    output <- numer/denom
    
    # Return ----
    return(output)
}

InitialiserHe <- function(nodes_in, order=2) {
    #' @title He Initialisation
    #' @description Initialise the weights based on Xavier initialisation
    #' @note Add a note for the developer.
    #' @references https://towardsdatascience.com/weight-initialization-in-neural-networks-a-journey-from-the-basics-to-kaiming-954fb9b47c79
    #' @references http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf
    #' @param nodes_in integer. The number of nodes coming in to this layer (ie. number of nodes in previous layer).
    #' @param order integer. The order of magnitude for this equation. Defaults to `2`.
    #' @return A floating point number.
    #' @author chrimaho
    
    # Validations ----
    assert_that(IsWhole(nodes_in))
    assert_that(IsWhole(order))
    
    # Do work ----
    numer <- order
    denom <- nodes_in
    output <- sqrt(numer/denom)
    
    # Return ----
    return(output)
}
```

## Layer Initialisation

```{r Layer Initialisation, eval=TRUE, echo=TRUE}
InitialiseLayer <- function(network_model, layer_index, initialisation_algorithm=NA, initialisation_order=2) {
    #' @title Initialise Layer
    #' @description Update the weights and biases of a given layer
    #' @note Defaults the initialisation algorithm to the Xavier Initialisation.
    #' @param network_model list. The model to be updated.
    #' @param layer_index integer. The layer to be updated.
    #' @param initialisation_algorithm string. The algorithm to be used for initialisation (eg. `xavier` or `he`).
    #' @param initialisation_order integer. The order of magnitude for the initialisation (can be either an integer, or set to the number of layers defined).
    #' @return The updated model, with the relevant layer updated.
    #' @author chrimaho
    
    # Validations
    assert_that(is_list(network_model))
    assert_that(IsWhole(layer_index))
    assert_that(initialisation_algorithm %in% c("xavier", "he", NA), msg="'initialisation_algorithm' must be one of 'xavier', 'he', or 'NA'.")
    assert_that(IsWhole(initialisation_order))
    
    # Get layer names
    layer_prev <- names(network_model)[layer_index-1]
    layer <- names(network_model)[layer_index]
    
    # Get number of nodes
    if (layer_index == 1) {
        nodes_in <- 0 #The first layer is the 'input' layer; therefore, there are 0 nodes feeding in to it.
    } else {
        nodes_in <- network_model %>% extract2(layer_prev) %>% extract2("nodz")
    }
    nodes_out <- network_model %>% extract2(layer) %>% extract2("nodz")
    
    # Initialise weight matrix
    w_matrix <- matrix(
        data=rnorm(nodes_in * nodes_out), 
        nrow=nodes_in,
        ncol=nodes_out
    )
    
    # Scale weights
    if (layer_index != 1) {
        if (initialisation_algorithm == "xavier") {
            w_matrix <- w_matrix * InitialiserXavier(nodes_in, nodes_out, order=initialisation_order)
        } else if (initialisation_algorithm == "he") {
            w_matrix <- w_matrix * InitialiserHe(nodes_in, order=initialisation_order)
        } else {
            w_matrix <- w_matrix
        }
    }
    
    # Initialise bias matrix
    b_matrix <- matrix(
        data=network_model %>% extract2(layer) %>% extract2("nodz") %>% replicate(0),
        nrow=network_model %>% extract2(layer) %>% extract2("nodz"),
        ncol=1
    )
    
    # Place data back in to the model
    network_model[[layer]][["wgts"]] <- w_matrix
    network_model[[layer]][["bias"]] <- b_matrix
    
    # Return
    return(network_model)
}
```

## Model Initialisation

```{r Model Initialisation, eval=TRUE, echo=TRUE, warning=FALSE}
InitialiseModel <- function(network_model, initialisation_algorithm="xavier", initialisation_order="layers") {
    #' @title Initialise Model
    #' @description Initialise each layer in the model.
    #' @note 
    #' @param network_model list. The model to be initialised.
    #' @param initialisation_algorithm string. The initialisation algorithm to be used.
    #' @param initialisation_order string or integer. The order of magnitude for the initialisation (either integer or set to the number of layers defined) 
    #' @return The initialised model.
    #' @author chrimaho
    
    # Validations
    assert_that(is_list(network_model))
    assert_that(initialisation_algorithm %in% c("xavier", "he", NA), msg="'initialisation_algorithm' must be one of 'xavier', 'he', or 'NA'.")
    assert_that(or(IsWhole(initialisation_order), is.string(initialisation_order)), msg="'initialisation_order' must be either type 'integer' or 'string'.")
    
    # Redefine 'initialisation_order'
    if (initialisation_order == "layers") {
        initialisation_order <- get_CountOfElementsWithCondition(names(network_model), function(x){IsWhole(as.numeric(x))})
    }
    
    # Initialise each layer
    for (layer_index in 1:length(names(network_model))) {
        network_model <- InitialiseLayer(
            network_model=network_model, 
            layer_index=layer_index, 
            initialisation_algorithm=initialisation_algorithm,
            initialisation_order=initialisation_order
        )
    }
    
    # Return
    return(network_model)
}
```

For this, the custom function `get_CountOfElementsWithCondition()` is used. The source code for the function can be found [here]().

## RUN

```{r Run the Setup, eval=TRUE, echo=TRUE}
network_model <- InitialiseModel(network_model)
```

# Forward Propogation

## The Theory

Check: [Matrix Manipulation](http://matrixmultiplication.xyz/)

$$

Multiply\ Matrices:

\begin{bmatrix}
1 &  9 & 17\\
2 & 10 & 18\\
3 & 11 & 19\\
4 & 12 & 20\\
5 & 13 & 21\\
6 & 14 & 22\\
7 & 15 & 23\\
8 & 16 & 24\\
\end{bmatrix}

\times

\begin{bmatrix}
1 & 4 & 7 & 10 & 13\\
2 & 5 & 8 & 11 & 14\\
3 & 6 & 9 & 12 & 15\\
\end{bmatrix}

=

\begin{bmatrix}
 70 & 151 & 232 & 313 & 394\\
 76 & 166 & 256 & 346 & 436\\
 82 & 181 & 280 & 379 & 478\\
 88 & 196 & 304 & 412 & 520\\
 94 & 211 & 328 & 445 & 562\\
100 & 226 & 352 & 478 & 604\\
106 & 241 & 376 & 511 & 646\\
112 & 256 & 400 & 544 & 688\\
\end{bmatrix}

\\

Add\ Biases:

\begin{bmatrix}
 70 & 151 & 232 & 313 & 394\\
 76 & 166 & 256 & 346 & 436\\
 82 & 181 & 280 & 379 & 478\\
 88 & 196 & 304 & 412 & 520\\
 94 & 211 & 328 & 445 & 562\\
100 & 226 & 352 & 478 & 604\\
106 & 241 & 376 & 511 & 646\\
112 & 256 & 400 & 544 & 688\\
\end{bmatrix}

+

\begin{bmatrix}
1\\
2\\
3\\
4\\
5\\
6\\
7\\
8\\
\end{bmatrix}

=

\begin{bmatrix}
 71 & 152 & 233 & 314 & 395\\
 78 & 168 & 258 & 348 & 438\\
 85 & 184 & 283 & 382 & 481\\
 92 & 200 & 308 & 416 & 524\\
 99 & 216 & 333 & 450 & 567\\
106 & 232 & 358 & 484 & 610\\
113 & 248 & 383 & 518 & 653\\
120 & 264 & 408 & 552 & 696\\
\end{bmatrix}

\\

Apply\ Activation:

\begin{bmatrix}
 71 & 152 & 233 & 314 & 395\\
 78 & 168 & 258 & 348 & 438\\
 85 & 184 & 283 & 382 & 481\\
 92 & 200 & 308 & 416 & 524\\
 99 & 216 & 333 & 450 & 567\\
106 & 232 & 358 & 484 & 610\\
113 & 248 & 383 & 518 & 653\\
120 & 264 & 408 & 552 & 696\\
\end{bmatrix}

\times

\delta

=

\begin{bmatrix}
 50.41 & 231.04 &  542.89 &  985.96 & 1560.25\\
 60.84 & 282.24 &  665.64 & 1211.04 & 1918.44\\
 72.25 & 338.56 &  800.89 & 1459.24 & 2313.61\\
 84.64 & 400.00 &  948.64 & 1730.56 & 2745.76\\
 98.01 & 466.56 & 1108.89 & 2025.00 & 3214.89\\
112.36 & 538.24 & 1281.64 & 2342.56 & 3721.00\\
127.69 & 615.04 & 1466.89 & 2683.24 & 4264.09\\
144.00 & 696.96 & 1664.64 & 3047.04 & 4844.16\\
\end{bmatrix}

$$

```{r}
mat_a <- matrix(1:24, 8, 3, byrow=TRUE)
mat_a

mat_b <- matrix(1:15, 3, 5, byrow=TRUE)
mat_b

mat_c <- mat_a %*% mat_b
mat_c

mat_d <- matrix(1:8, 8, 1)
mat_d

mat_e <- sweep(mat_c, 1, mat_d, "+")
mat_e

mat_f <- mat_e*(0.01*mat_e)
mat_f

rm(mat_a, mat_b, mat_c, mat_d, mat_e, mat_f)
```

## Linear Component

```{r}
LinearForward <- function(inpt, wgts, bias) {
    
    linr <- inpt %*% wgts
    linr <- sweep(linr, 2, bias, "+")
    return(linr)
    
}
```

## Non-Linear Component

Check: [Activation Functions](https://www.desmos.com/calculator/rhx5tl8ygi)

```{r}
sigmoid <- function(z) {
    # References:
    # https://kite.com/python/answers/how-to-calculate-a-logistic-sigmoid-function-in-python
    # https://www.geeksforgeeks.org/implement-sigmoid-function-using-numpy/
    a <- 1/(1+exp(-z))
    return(a)
}

relu <- function(z) {
    # References:
    # https://medium.com/ai%C2%B3-theory-practice-business/a-beginners-guide-to-numpy-with-sigmoid-relu-and-softmax-activation-functions-25b840a9a272
    a <- sapply(z, max, 0) %>% 
        structure(dim=dim(z))
    return(a)
}

softmax <- function(z) {
    # Reference: 
    # https://medium.com/ai%C2%B3-theory-practice-business/a-beginners-guide-to-numpy-with-sigmoid-relu-and-softmax-activation-functions-25b840a9a272
    expo <- exp(z)
    expo_sum <- sum(exp(z))
    a <- expo/expo_sum
    return(a)
}

swish <- function(z, beta=0.1) {
    # References: 
    # https://arxiv.org/pdf/1710.05941.pdf
    # https://www.bignerdranch.com/blog/implementing-swish-activation-function-in-keras/
    a <- z * (beta*z)
    return(a)
}

```

## Forward Prop

```{r}
ForwardProp <- function(data_in, network_model, activation_hidden="relu", activation_final="sigmoid") {
    
    # Validations
    assert_that(is.array(data_in))
    assert_that(is.list(network_model))
    assert_that(names(network_model)[1]=="input")
    assert_that(rev(names(network_model))[1]=="output")
    assert_that(is.string(activation_hidden))
    assert_that(is.string(activation_final))
    assert_that(activation_hidden %in% c("sigmoid","relu","softmax","swish"))
    assert_that(activation_final %in% c("sigmoid","relu","softmax","swish"))
    for (name in names(network_model)) {
        if (!name %in% c("input","output")) {
            assert_that(IsWhole(as.numeric(name)))
        }
    }
    
    # Do work
    for (index in 1:length(names(network_model))) {
        
        # Define layer name
        layr <- names(network_model)[index]
        
        if (layr=="input") {
            
            # Pass-thru for 'input' layer
            network_model[[layr]][["inpt"]] <- data_in
            network_model[[layr]][["acti"]] <- data_in
            
        } else {
            
            # Extract data
            prev <- names(network_model)[index-1]
            inpt <- network_model[[prev]][["acti"]]
            wgts <- network_model[[layr]][["wgts"]]
            bias <- network_model[[layr]][["bias"]]
            
            # Calculate
            linr <- LinearForward(inpt, wgts, bias)
            
            # Activate
            if (layr=="output") {
                acti <- get(activation_final)(linr)
                network_model[[layr]][["acti_func"]] <- activation_final
            } else {
                acti <- get(activation_hidden)(linr)
                network_model[[layr]][["acti_func"]] <- activation_hidden
            }
            
            # Apply back to our model
            network_model[[layr]][["inpt"]] <- inpt
            network_model[[layr]][["linr"]] <- linr
            network_model[[layr]][["acti"]] <- acti
            
        }
        
    }
    
    # Return
    return(network_model)
}
```

## Run

```{r}
tic()
network_model <- ForwardProp(trn_img, network_model, "relu", "sigmoid")
toc()
```

# Calculate the Cost

## Set Up

```{r}
ComputeCost <- function(pred, true, epis=1e-10) {
    
    # Validations
    assert_that(is.matrix(pred))
    assert_that(is.matrix(true))
    assert_that(is.number(epis))
    assert_that(epis < 0.0001, msg="'epis' should be a very small epsilom value.")
    
    # Get number of samples
    samp <- length(true)
    
    # Instantiate totals
    total_cost <- 0
    
    # Loop for each prediction
    for (i in 1:samp) {
        
        # Adjust for perfect predictions.
        pred <<- pred
        i <<- i
        if (pred[i]==1) {pred[i] <- pred[i]-epis} #pred[i] %<>% subtract(epis)
        if (pred[i]==0) {pred[i] <- pred[i]+epis} #pred[i] %<>% add(epis)
        
        # Calculate totals
        total_cost <- total_cost - ((true[i] * log(pred[i]) + (1-true[i]) * log(1-pred[i])))
        
    }
    
    # Take an average
    cost <- (1/samp) * total_cost
    
    # Return
    return(cost)
    
}

ApplyCost <- function(network_model, cost) {
    
    # Apply back to the model
    for (layer in names(network_model)) {
        network_model[[layer]][["cost"]] <- cost
    }
    
    return(network_model)
}
```

## Run

```{r}
network_model <- ApplyCost(network_model, ComputeCost(network_model[["output"]][["acti"]], trn_cls))
```

# Backward Propogation

## Differentiate Cost

### Set

```{r}
DifferentiateCost <- function(pred, true) {
    diff_cost <- -(divide_by(true, pred) - divide_by(1-true, 1-pred))
    return(diff_cost)
}

ApplyDifferentiateCost <- function(network_model, cost_differential) {
    for (layer in names(network_model)) {
        network_model[[layer]][["back_cost"]] <- cost_differential
        if (layer=="output") {
            network_model[[layer]][["back_acti"]] <- network_model[[layer]][["back_cost"]] %>% t()
        }
    }
    return(network_model)
}
```

### Run

```{r}
network_model <- ApplyDifferentiateCost(network_model, DifferentiateCost(network_model[["output"]][["acti"]], trn_cls))
```

## Differentiate Linear

```{r}
DifferentiateLinear <- function(back_linr_curr, acti_prev, wgts, bias) {
    
    samp <- dim(acti_prev)[2]
    
    # print(dim(back_linr_curr))
    # print(dim(acti_prev))
    # print(dim(wgts))
    
    diff_wgts <- 1/samp * (back_linr_curr %*% acti_prev)
    diff_bias <- 1/samp * rowSums(back_linr_curr, dims=1)
    diff_acti_prev <- wgts %*% back_linr_curr
    
    list_linr <- list(
        diff_acti_prev, 
        diff_wgts, 
        diff_bias
    )
    
    return(list_linr)
    
}
```

## Differentiate Activation

```{r}
relu_backward <- function(diff_acti_curr, linr_curr) {
    diff_linr_curr <- diff_acti_curr
    diff_linr_curr[linr_curr<=0] <- 0
    return(diff_linr_curr)
}

sigmoid_backward <- function(diff_acti_curr, linr_curr) {
    temp <- 1/(1+exp(-linr_curr))
    diff_linr_curr <- t(diff_acti_curr) * temp * (1-temp)
    return(t(diff_linr_curr))
}
```

## Run Backward Propogation

### Set

```{r}
BackwardProp <- function(network_model) {
    
    # Loop through each layer in reverse order
    for (layr_indx in network_model %>% names %>% length %>% 1:. %>% rev) {
        
        # Get the layer name
        layr_curr <- network_model %>% names %>% extract(layr_indx)
        
        # Skip the 'input' layer
        if (layr_curr == "input") next
        
        # Get the previous layer name
        layr_prev <- network_model %>% names %>% extract(layr_indx-1)
        
        # Set up the variables
        linr_curr <- network_model[[layr_curr]][["linr"]]
        wgts_curr <- network_model[[layr_curr]][["wgts"]]
        bias_curr <- network_model[[layr_curr]][["bias"]]
        acti_prev <- network_model[[layr_prev]][["acti"]]
        acti_func_back <- network_model[[layr_curr]][["acti_func"]] %>% paste0("_backward")
        diff_acti_curr <- network_model[[layr_curr]][["back_acti"]]
        diff_linr_curr <- matrix()
        diff_acti_prev <- matrix()
        diff_wgts_curr <- matrix()
        diff_bias_curr <- matrix()
        
        # Differentiate activation
        diff_linr_curr <- get(acti_func_back)(diff_acti_curr, linr_curr)
        
        # Differentiate linear
        list_linr <- DifferentiateLinear(
            back_linr_curr=diff_linr_curr,
            acti_prev=acti_prev,
            wgts=wgts_curr,
            bias=bias_curr
        )
        diff_acti_prev <- list_linr[[1]]
        diff_wgts_curr <- list_linr[[2]]
        diff_bias_curr <- list_linr[[3]]
        
        # Apply back to model
        network_model[[layr_prev]][["back_acti"]] <- diff_acti_prev
        network_model[[layr_curr]][["back_linr"]] <- diff_linr_curr
        network_model[[layr_curr]][["back_wgts"]] <- diff_wgts_curr
        network_model[[layr_curr]][["back_bias"]] <- diff_bias_curr
        
    }
    
    return(network_model)
    
}
```

### Run

```{r}
network_model <- BackwardProp(network_model)
```

# Update Model Parameters

```{r}
UpdateModel <- function(network_model, learning_rate) {
    
    for (index in 1:length(names(network_model))) {
        
        # Get layer name
        layr <- names(network_model)[index]
        
        # Skip 'input' layer
        if (layr=="input") next
        
        # Define gradient steps
        grad_step_wgts <- -1 * (learning_rate * network_model[[layr]][["back_wgts"]])
        grad_step_bias <- -1 * (learning_rate * network_model[[layr]][["back_bias"]])
        
        # Take steps
        network_model[[layr]][["wgts"]] <- network_model[[layr]][["wgts"]] + t(grad_step_wgts)
        network_model[[layr]][["bias"]] <- network_model[[layr]][["bias"]] + grad_step_bias
        
    }
    
    return(network_model)
    
}
```

```{r}
network_model <- UpdateModel(network_model, 0.01)
```

# Run the model

```{r}
TrainModel <- function(x_train, y_train,
                       input_nodes=dim(x_train)[2], hidden_nodes=c(100, 50, 10), output_nodes=1,
                       initialisation_algorithm="xavier", initialisation_order="layers",
                       epochs=500, learning_rate=0.001,
                       activation_hidden="relu", activation_final="sigmoid",
                       verbosity=NA
                       ) {
    
    # Instantiate
    network_model <- InstantiateNetwork(
        input=input_nodes,
        hidden=hidden_nodes, 
        output=output_nodes
    )
    
    # Initialise
    network_model <- InitialiseModel(
        network_model=network_model, 
        initialisation_algorithm=initialisation_algorithm, 
        initialisation_order=initialisation_order
    )
    
    # Loop each epoch
    for (epoch in 1:epochs) {
        
        # Forward Prop
        network_model <- ForwardProp(
            data_in=x_train, 
            network_model=network_model, 
            activation_hidden=activation_hidden, 
            activation_final=activation_final
        )
        
        # Get cost
        cost <- ComputeCost(network_model[["output"]][["acti"]], y_train, 1e-10)
        
        # Apply cost
        network_model <- ApplyCost(
            network_model=network_model, 
            cost=cost
        )
        
        # Print cost
        if (!is.na(verbosity)) {
            if (epoch %% verbosity == 0) {
                print("With learning rate {}, at epoch {}, the cost is: {}" %>% str_Format(learning_rate, epoch, cost))
            }
        }
        
        # Differentiate cost
        network_model <- ApplyDifferentiateCost(
            network_model=network_model, 
            DifferentiateCost(network_model[["output"]][["acti"]], y_train)
        )
        network_model <<- network_model
        
        # Backprop
        network_model <- BackwardProp(network_model)
        
        # Update parameters
        network_model <- UpdateModel(
            network_model=network_model, 
            learning_rate=learning_rate
        )
        
    }
    
    return(network_model)
}
```

```{r}
network_model <- TrainModel(
    x_train=trn_img, y_train=trn_cls,
    input_nodes=dim(trn_img)[2], hidden_nodes=c(100,75,50,30,20), output_nodes=1,
    initialisation_algorithm="xavier", initialisation_order="layers",
    epochs=20, learning_rate=0.001,
    activation_hidden="relu", activation_final="sigmoid",
    verbosity=1
)
```

```{r}
dim(trn_img)
```

